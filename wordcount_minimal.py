import logging
import os
import re

from past.builtins import unicode

import apache_beam as beam

from apache_beam.options.pipeline_options import (
    PipelineOptions,
    GoogleCloudOptions,
    StandardOptions,
    SetupOptions,
)

from apache_beam.io import ReadFromText, WriteToText
from dotenv import load_dotenv

load_dotenv()

GOOGLE_CLOUD_PROJECT = os.getenv("GCP_PROJECT", None)
GCP_BUCKET = os.getenv("GCP_BUCKET", None)


def pipeline_options() -> PipelineOptions:
    options = PipelineOptions()
    gcp_options = options.view_as(GoogleCloudOptions)
    gcp_options.project = GOOGLE_CLOUD_PROJECT
    gcp_options.job_name = "wordcount-minimal"
    gcp_options.staging_location = f"gs://{GCP_BUCKET}/staging"
    gcp_options.temp_location = f"gs://{GCP_BUCKET}/temp"
    gcp_options.region = "eu-north-1"
    # options.view_as(StandardOptions).runner = "DataflowRunner"
    gcp_options.view_as(SetupOptions).save_main_session = True
    return options


def main():
    options = pipeline_options()

    input_file = "gs://dataflow-samples/shakespeare/kinglear.txt"
    output_file = f"gs://{GCP_BUCKET}/output/wordcount-minimal"

    with beam.Pipeline(options=options) as p:

        # "A text file Read transform is applied to the Pipeline object itself,
        # and produces a PCollection as output. Each element in the output PCollection
        # represents one line of text from the input file."
        lines = p | ReadFromText(input_file)

        counts = (
            lines
            # "This transform splits the lines in PCollection<String>, where each element
            # is an individual word in Shakespeareâ€™s collected texts. As an alternative,
            # it would have been possible to use a ParDo transform that invokes a DoFn
            # (defined in-line as an anonymous class) on each element that tokenizes the
            # text lines into individual words. The input for this transform is the PCollection
            # of text lines generated by the previous TextIO.Read transform. The ParDo transform
            # outputs a new PCollection, where each element represents an individual word in the text."
            | "Split"
            >> (
                beam.FlatMap(lambda x: re.findall(r"[A-Za-z\']+", x)).with_output_types(
                    unicode
                )
            )
            | "PairWithOne" >> beam.Map(lambda x: (x, 1))
            | "GroupAndSum" >> beam.CombinePerKey(sum)
        )

        def format_result(word_count):
            (word, count) = word_count
            return "%s: %s" % (word, count)

        output = counts | "Format" >> beam.Map(format_result)
        output | WriteToText(output_file)


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)
    main()
